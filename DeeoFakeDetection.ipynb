{"cells":[{"metadata":{"id":"AfgmhvXCUIZz"},"cell_type":"markdown","source":"<h1>Deepfake Detection Challenge</h1>\n\n<h4>Biometric Systems 2019/2020 Project</h4>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations==0.4.5\n!pip install efficientnet\n!pip install facenet-pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.colab import files\nfiles.upload()\n\n!pip install -q kaggle\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!ls ~/.kaggle\n!chmod 600 /root/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"id":"37zx4VvTcFZ3","outputId":"ae463ec6-9048-41d2-df28-9bddd4a4077d","trusted":true},"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","execution_count":null,"outputs":[]},{"metadata":{"id":"OWJqW-dIV9md","outputId":"511e6f22-8ef8-458d-d9b9-a14004aff9d6","trusted":true},"cell_type":"code","source":"!kaggle datasets download -d meraxes10/datadfdc\n!mkdir datadfdc\n!unzip datadfdc.zip -d ./datadfdc\n!rm datadfdc.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"JurSb1rgcVe5","outputId":"4b2226d5-1fdf-4495-d800-9f7d4ec7a439","trusted":true},"cell_type":"code","source":"!kaggle datasets download -d meraxes10/dfdctrainset\n!mkdir dfdctrainset\n!unzip dfdctrainset.zip -d ./dfdctrainset\n!rm dfdctrainset.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Py6p1QZKUIZ6"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DRDBCnWqUIZ9"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sOWgQyA3UIaA"},"cell_type":"code","source":"import gc\nimport random\nimport time\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sEF3fVNtUIaE"},"cell_type":"code","source":"import cv2\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"7czm27LcUIaI"},"cell_type":"code","source":"import zipfile\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"iSfCELUzUIaL"},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"id":"m45VTK9YUdtB","outputId":"57cd8e39-6fdf-4370-8fb7-85c83adfdf19","trusted":true},"cell_type":"code","source":"torch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"aGiTTS9WUOFn","trusted":true},"cell_type":"code","source":"from facenet_pytorch import MTCNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"-lx77B6MUIaO"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"iwY4b57MUIaR"},"cell_type":"code","source":"from tensorflow.keras import optimizers\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8QA7XJP6UIaV"},"cell_type":"code","source":"import albumentations\nfrom albumentations.augmentations import transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"bkhyCkdFUIaY"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WOcmeYDVUIab"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"xO9LumrqUIag"},"cell_type":"code","source":"def unzip_videos(orig, dest):\n    for root, dirs, files in os.walk(orig):\n        for file in files:\n            zipname = root + '/' + file\n            if zipfile.is_zipfile(zipname):  \n                with zipfile.ZipFile(zipname,\"r\") as zip_ref:\n                    zip_ref.extractall(dest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"NH2TQO-wUIai"},"cell_type":"code","source":"def extract_metadata(dest):\n    for i in range(0, 50):\n        df = pd.read_json('./train_set_videos/dfdc_train_part_' + str(i) + '/metadata.json')\n        try:\n            os.mkdir(dest + '/dfdc_train_part_' + str(i))\n        except OSError:\n            print(\"Creation of the directory failed!\")\n            return\n        df.to_json(dest + '/dfdc_train_part_' + str(i) + '/metadata.json')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"id":"-DqEg8M3UIal"},"cell_type":"code","source":"def extract_face_from_frame(frame, fd):\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    pilimg = Image.fromarray(frame)\n    # face detection and extraction\n    faces, confs = fd.detect(pilimg)\n    if faces is None:\n        return None\n    best = confs.argmax()\n    box = [max(0, int(x)) for x in faces[best].tolist()]\n    img = frame[box[1]:box[3], box[0]:box[2]]\n\n    #resize and 0 border\n    sf = 224/np.max(img.shape)\n    img_rs = cv2.resize(img, (int(img.shape[1]*sf), int(img.shape[0]*sf)), fx=sf, fy=sf)\n    bottom = int((224-int(img.shape[0]*sf))/2)\n    top = 224 - img_rs.shape[0] - bottom\n    left = int((224-int(img.shape[1]*sf))/2)\n    right = 224 - img_rs.shape[1] - left\n    img = cv2.copyMakeBorder(img_rs, top, bottom, left, right, 0)\n    return img","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"id":"aXrSKTJkUIan"},"cell_type":"code","source":"def extract_faces_from_video(path, fd, t=None, n=None, transforms=None):\n    output = []\n    cap = cv2.VideoCapture(path)\n    ret = True\n    begin = time.time()\n    count = 0\n    while ret:\n        if n is not None and count >= n:\n            break\n        if (not t is None) and (time.time() - begin > t):\n            break\n        ret = cap.grab()\n        if not ret:\n            break\n            \n        # next frame extraction\n        ret, frame = cap.retrieve()\n        \n        img = extract_face_from_frame(frame, fd, transforms=None)\n        if transforms is None:\n            img = cv2.cvtColor(img_rs, cv2.COLOR_RGB2BGR)\n        else:\n            img = transforms(image=img_rs)['image']\n        if img is None:\n            continue\n        \n        output.append(img)\n        count += 1\n    cap.release()\n    return np.asarray(output)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"id":"B3P2yLJYUIaq"},"cell_type":"code","source":"def preprocess_videos(n, begin, end):\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    mtcnn = MTCNN(keep_all=False, select_largest=False, device=device, min_face_size = 60)\n    for i in range(begin, end):\n        df = pd.read_json('./train_set_videos/dfdc_train_part_' + str(i) + '/metadata.json')\n        print(str(i), 'TOT:', df.shape[1])\n        try:\n            os.mkdir('./train_set_faces/dfdc_train_part_' + str(i))\n        except OSError:\n            print(\"Creation of the directory failed!\")\n            break\n        count = 0\n        for index, row in df.transpose().iterrows():\n            filename = str(index).split('.')[0]\n            try:\n                os.mkdir('./train_set_faces/dfdc_train_part_' + str(i) + '/' + filename)\n            except OSError:\n                print(\"Creation of the directory failed!\")\n                break            \n            faces = extract_face('./train_set_videos/dfdc_train_part_' + str(i) + '/' + index, mtcnn, n)\n            print(count)\n            if len(faces) == 0:\n                continue\n            j = 0\n            for face in faces:\n                path = './train_set_faces/dfdc_train_part_' + str(i) + '/' + filename + '/' + filename + '_' + str(j) + '.png'\n                cv2.imwrite(path, face)\n                j += 1\n            count += 1\n        print(str(i), 'EXT:', count)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"id":"-mba20qdUIat"},"cell_type":"code","source":"def train_valid_split(filename, valid_balanced=True):\n    train_data = pd.read_csv(filename)\n    train_data = train_data[['video', 'face', 'original', 'chunk', 'label']]\n\n    train_data['split'] = 'train'\n\n    train_data.loc[((train_data.chunk >= 40) & (train_data.chunk < 50)), 'split'] = 'valid'\n\n    valid_set = train_data[train_data.split == 'valid']\n    train_set = train_data[train_data.split == 'train']\n\n    valid_set.drop(columns='split', inplace=True)\n    train_set.drop(columns='split', inplace=True)\n\n    temp = train_set[train_set.label == 'FAKE'][['video', 'face', 'original']]\n    temp.rename(columns={'video': 'fake', 'face': \n                         'face_fake', 'original': 'video'}, inplace=True)\n\n    train_set = temp.merge(train_set[train_set.label == 'REAL'], how='left', on='video')\n\n    train_set.dropna(subset=['face'], inplace=True)\n\n    temp = valid_set[valid_set.original.isin(valid_set.loc[valid_set.label == 'REAL', 'video'].tolist())]\n\n    if valid_balanced:\n        temp = temp.groupby(['original']).apply(lambda x : x.sample(1, replace=False, random_state=42))\n    temp.reset_index(inplace=True, drop=True)\n\n    valid_set = temp.append(valid_set[valid_set.label == 'REAL'])\n    valid_set = valid_set.sample(frac=1).reset_index(drop=True)\n    return train_set, valid_set","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"id":"5DEH5epMUIav"},"cell_type":"code","source":"def get_transfoms():\n    train_transforms = albumentations.Compose([     \n        transforms.ShiftScaleRotate(p=0.2, scale_limit=0.25, \n                                    border_mode=1, rotate_limit=25),\n        transforms.HorizontalFlip(p=0.1),\n        transforms.Cutout(p=.1),\n        transforms.RandomContrast(p=.1),\n        transforms.RandomBrightness(p=.1, limit=0.3),\n        transforms.JpegCompression(p=.2, quality_lower=15, quality_upper=60),\n        transforms.Downscale(scale_min=0.25, scale_max=0.25, p=0.2),\n        transforms.GaussNoise(p=0.1),\n        transforms.Normalize()\n    ], additional_targets={'image2': 'image'})\n    \n    valid_transforms = albumentations.Compose([\n        transforms.Normalize()\n    ])\n    \n    return train_transforms, valid_transforms","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"id":"Hxi3R2Z7UIay"},"cell_type":"code","source":"def get_test_transfoms():\n    test_transforms = albumentations.Compose([\n            transforms.RandomBrightness(p=1.0, limit=0.3),\n            transforms.Normalize()\n        ])\n    return test_transforms","execution_count":10,"outputs":[]},{"metadata":{"id":"Fid8qdU0uAXQ","trusted":true},"cell_type":"code","source":"def generate(dataset, batch_size, transforms, root_dir, train=True):\n    X, y = [], []\n    for index, row in dataset.iterrows():\n        img = cv2.imread(root_dir + str(row.face))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if train:\n            img_fake = cv2.imread(root_dir + str(row.face_fake))\n            img_fake = cv2.cvtColor(img_fake, cv2.COLOR_BGR2RGB)\n            img = transforms(image=img, image2=img_fake)\n        else:\n            img = transforms(image=img)\n\n        X.append(img['image'])\n        y.append((1 if str(row.label) == 'FAKE' else 0))\n        \n        if train:\n            X.append(img['image2'])\n            y.append(1)\n\n        if len(X) == batch_size:\n            yield np.array(X), np.array(y)\n            X, y = [], []","execution_count":11,"outputs":[]},{"metadata":{"id":"bGo7e8NzbSky","trusted":true},"cell_type":"code","source":"def evaluate(models, valid_set, batch_size, transforms, root_dir, clips, verbose=0):\n    loss = keras.losses.BinaryCrossentropy()\n\n    y_pred = np.array([])\n    target = np.array([])\n    for X_valid, Y_valid in tqdm(generate(valid_set, batch_size, \n                                          transforms, root_dir, train=False)):\n        target = np.append(target, Y_valid, axis=0)\n        outputs = np.zeros(batch_size)\n        for model in models:\n            outputs += (model.predict(np.array(X_valid), \n                                      batch_size=batch_size).reshape(-1) / len(models))\n\n        y_pred = np.append(y_pred, outputs, axis=0)\n    \n    valid_loss = loss(target, y_pred).numpy()\n\n    clipped_losses = np.array([])\n    for low, high in clips:\n        clip_loss = loss(target, np.clip(y_pred, low, high)).numpy()\n        clipped_losses = np.append(clipped_losses, [clip_loss], axis=0)\n            \n    acc = accuracy_score(target, np.round(y_pred))\n    auc = roc_auc_score(target, y_pred)\n    cm = confusion_matrix(target, np.round(y_pred))\n    curve = roc_curve(target, y_pred)\n    if verbose:\n        print(classification_report(target, np.round(y_pred)))\n    return valid_loss, acc, auc, cm, clipped_losses, curve","execution_count":12,"outputs":[]},{"metadata":{"id":"R38ix2QTn9gF","trusted":true},"cell_type":"code","source":"def train(model, name, train_set, valid_set, early_stopping, \n          train_transforms, valid_transforms, root_dir,\n          batch_size, epochs):\n    model_name = 'checkpoint_' + name + '.h5'\n    patience = 0\n    best_val_loss = None\n    history = list()\n    for epoch_n in range(0, epochs):\n        print('EPOCH:', epoch_n, '/', epochs)\n        train_set = train_set.sample(frac=1).reset_index(drop=True)\n\n        nbatches = 0\n        train_loss = 0\n        train_acc = 0\n        for i, (X_train, Y_train) in tqdm(enumerate(generate(train_set, batch_size,\n                                                             train_transforms, root_dir, \n                                                             train=True), 1)):\n            batch_loss, batch_acc = model.train_on_batch(X_train, Y_train, \n                                                         reset_metrics=True)\n            train_loss += batch_loss\n            train_acc += batch_acc\n            nbatches += 1\n\n            if i % 1000 == 0:\n                train_loss /= nbatches\n                train_acc /= nbatches\n                nbatches = 0\n                # evaluate\n                valid_loss, val_acc, val_auc, val_cm, clipped_loss, _ = evaluate([model], valid_set, 32,\n                                                                                  valid_transforms, root_dir, \n                                                                                  [(0.1, 0.9), (0.15, 0.85)])\n                if (best_val_loss is None) or (valid_loss < best_val_loss):\n                    patience = 0\n                    best_val_loss = valid_loss\n                    model.save_weights(model_name)\n                else:\n                    patience += 1\n\n                print('\\nTRAIN LOSS:', train_loss, 'VALID LOSS:', valid_loss, \n                      'VALID LOSS CLIPPED:', clipped_loss)\n                history.append({'epoch': epoch_n, 'chunk': i, 'train_loss': train_loss, \n                                'valid_loss': valid_loss, \"val_loss_clip\": clipped_loss,\n                                'valid_accuracy': val_acc, 'valid_auc': val_auc,\n                                'valid_cm': val_cm})\n\n                # early stopping\n                if patience >= early_stopping:\n                    model.load_weights(model_name)\n                    return model, history\n    model.load_weights(model_name)\n    return model, history","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    base_model = efn.EfficientNetB7(weights='imagenet',\n                                     include_top=False,\n                                     pooling='avg', \n                                     input_shape=(224,224,3))\n\n    x = base_model.output\n    predicted = Dense(1,activation ='sigmoid')(x)\n\n    model = Model(inputs=base_model.input, outputs=predicted)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Ttr6RIybUIbB"},"cell_type":"code","source":"def load_models():\n    eff_model1 =  efn.EfficientNetB7(weights=None,\n                                     include_top=False,\n                                     pooling='avg', \n                                     input_shape=(224,224,3))\n    x = eff_model1.output\n    predicted = Dense(1,activation ='sigmoid')(x)\n    eff_model1 = Model(inputs=eff_model1.input, outputs=predicted)\n    eff_model1.load_weights('datadfdc/effic_student_weights.h5')\n\n    eff_model2 =  efn.EfficientNetB7(weights=None,\n                                     include_top=False,\n                                     pooling='avg', \n                                     input_shape=(224,224,3))\n    x = eff_model2.output\n    x = Dense(128,activation ='relu')(x)\n    x = Dense(64,activation ='relu')(x)\n    predicted = Dense(1,activation ='sigmoid')(x)\n    eff_model2 = Model(inputs=eff_model2.input, outputs=predicted)\n    eff_model2.load_weights('datadfdc/effic_best_acc_weights (1).h5')\n\n    eff_model3 =  efn.EfficientNetB7(weights=None,\n                                     include_top=False,\n                                     pooling='avg', \n                                     input_shape=(224,224,3))\n    x = eff_model3.output\n    predicted = Dense(1,activation ='sigmoid')(x)\n    eff_model3 = Model(inputs=eff_model3.input, outputs=predicted)\n    eff_model3.load_weights('datadfdc/effic_sgd_weights.h5')\n\n    xcep_model =  keras.applications.xception.Xception(weights=None,\n                                     include_top=False,\n                                     pooling='avg', \n                                     input_shape=(224,224,3))\n    x = xcep_model.output\n    predicted = Dense(1,activation ='sigmoid')(x)\n    xcep_model = Model(inputs=xcep_model.input, outputs=predicted)\n    xcep_model.load_weights('datadfdc/xcep_weights.h5')\n\n    models = [eff_model1, eff_model2, eff_model3, xcep_model]\n\n    for model in models:\n        model.compile(loss='binary_crossentropy',\n                       optimizer=keras.optimizers.Adam(learning_rate=0.00005),\n                       metrics=[metrics.binary_accuracy])    \n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"aPYBbxLFUIbE"},"cell_type":"code","source":"def predict(models, df, root_dir, d_type='video', extract='No'):\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    mtcnn = MTCNN(keep_all=False, select_largest=False, device=device, min_face_size = 60)\n    test_transforms = get_test_transfoms()\n    preds = []\n    b = time.time()\n    for index, row in tqdm(df.iterrows()):\n        if d_type == 'video':\n            imgs = extract_faces_from_video(root_dir + row.filename, mtcnn, t=1, \n                                            transforms=test_transforms)\n        else:\n            img = cv2.imread(root_dir + row.face)\n            if extract == 'Yes':\n                img = extract_face_from_frame(img, mtcnn)\n            else:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if len(img) == 0:\n                preds.append(0.51)\n                continue                \n            imgs = test_transforms(image=img)['image'].reshape((-1, 224, 224, 3))\n        if len(imgs) == 0:\n            preds.append(0.51)\n            continue\n        output = 0\n        for model in models:\n            output += model.predict(imgs).mean() / len(models)\n        preds.append(np.clip(output, 0.01, 0.99))\n        print(index, len(imgs), output)\n    print(time.time() - b)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EVEavVLkUIbJ"},"cell_type":"code","source":"def simple_predict(models, filename):\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    mtcnn = MTCNN(keep_all=False, select_largest=False, device=device, min_face_size = 60)\n    test_transforms = get_test_transfoms()\n    img = cv2.imread(filename)\n\n    img_crop = extract_face_from_frame(img, mtcnn)\n\n    if len(img_crop) == 0:\n        return 0.51\n    img = test_transforms(image=img_crop)['image'].reshape((-1, 224, 224, 3))\n    output = 0\n    \n    for model in models:\n        output += model.predict(img).mean() / len(models)\n    plt.xlabel('FAKE; PREDICTION:' + \"{:.2f}\".format(output))\n    plt.imshow(img_crop)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"j4M59jWOUIbM"},"cell_type":"code","source":"def plot_confusion_matrix(cm):\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g'); \n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    ax.set_title('Confusion Matrix')\n    ax.xaxis.set_ticklabels(['REAL', 'FAKE'])\n    ax.yaxis.set_ticklabels(['REAL', 'FAKE'])   \n    plt.savefig('cm.png')\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"6m0mrATVUIbO"},"cell_type":"code","source":"def plot_roc_curve(x, y, auc):\n    plt.plot(x, y, color='darkorange', lw=2, label='ROC curve (area = %0.3f)' % auc)\n    plt.ylabel('1 - FRR')\n    plt.xlabel('FAR')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('roc.png')\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"eXCvTp5OUIbQ"},"cell_type":"code","source":"global models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"QQhhG4s1UIbT"},"cell_type":"code","source":"models = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sx4x1XyIUIbV"},"cell_type":"code","source":"def main(**args):\n    print(args)\n    global models\n    #unzip_videos(\"../dfdc_down\", \"../dfdc/train_set_videos\")\n    #extract_metadata('./train_metadata')\n    #preprocess_videos(1, 0, 50)\n    \n    PATH = args['--path']\n    \n    if torch.cuda.is_available():\n        print(torch.cuda.get_device_name(0))\n        \n    if args['--mode'] == 'train':\n        train_transforms, valid_transforms = get_transfoms()\n        train_set, valid_set = train_valid_split('dfdctrainset/train_data.csv')\n        model = build_model()\n        model.compile(loss='binary_crossentropy',\n                      optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.8, nesterov=False),\n                      metrics=[metrics.binary_accuracy])\n        best_val_loss, val_acc, val_auc, val_cm, clipped_loss, _ = evaluate([model], valid_set, 32, \n                                                                            valid_transforms, PATH, \n                                                                            [(0.1, 0.9), (0.15, 0.85)])\n        print(best_val_loss, val_acc, val_auc, clipped_loss)\n        EPOCHS = 10\n        EARLY_STOPPING = 5\n        BATCH_SIZE = 16\n        model, history = train(model, '0', train_set, valid_set, EARLY_STOPPING, \n                               train_transforms, valid_transforms, PATH, BATCH_SIZE, EPOCHS)\n        best_val_loss, val_acc, val_auc, val_cm, clipped_loss, _ = evaluate([model], valid_set, 32, \n                                                                            valid_transforms, PATH, \n                                                                            [(0.1, 0.9), (0.15, 0.85)])\n        print(best_val_loss, val_acc, val_auc, clipped_loss)\n    elif args['--mode'] == 'eval':    \n        if models is None:\n            models = load_models()\n            \n        _, valid_transforms = get_transfoms()\n        \n        train_set, valid_set = train_valid_split(args['--metadata'], valid_balanced=False)\n        \n        loss, acc, auc, cm, clipped_loss, curve = evaluate(models, valid_set, 32, valid_transforms, PATH, \n                                                           [(0.1, 0.9), (0.15, 0.85)], verbose=1)\n        \n        print('LOSS:', loss, 'ACCURACY:', acc, 'AUC:', auc, 'CLIPPED LOSSES:', clipped_loss)\n        plot_confusion_matrix(cm)\n        plot_roc_curve(curve[0], curve[1], auc)\n        return loss\n    elif args['--mode'] == 'predict':     \n        if models is None:\n            models = load_models()\n        \n        df = pd.read_csv(args['--metadata'])\n        \n        \n        if '--indices' in args:\n            df = df.loc[args['--indices']]\n        \n        submit = df\n\n        sub_preds = predict(models, df, PATH, d_type=args['--type'], extract=args['--extract'])\n\n        submit['label'] = sub_preds\n\n        submit.reset_index(inplace=True, drop=True)\n\n        submit.to_csv('submission.csv', index=False)\n        return submit\n    elif args['--mode'] == 'predict frame':\n        if models is None:\n            models = load_models()      \n        return simple_predict(models, PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"q5fMB-SXUIbY"},"cell_type":"code","source":"# train\nargs = {'--mode': 'train',\n        '--path': 'dfdctrainset/train_set_faces/train_set_faces/'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"0D18h-q_UIba"},"cell_type":"code","source":"# prediction\nargs = {'--mode': 'predict',\n        '--type': 'video',\n        '--metadata': 'deepfake-detection-challenge/sample_submission.csv',\n        '--path': 'deepfake-detection-challenge/test_videos/'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"3eLTK5RlUIbd"},"cell_type":"code","source":"# evaluation\nargs = {'--mode': 'eval',\n        '--metadata': 'dfdctrainset/train_data.csv',\n        '--path': 'dfdctrainset/train_set_faces/train_set_faces/'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ggXWysL-UIbf"},"cell_type":"code","source":"# subset evaluation\nargs = {'--mode': 'predict',\n        '--type': 'image',\n        '--extract': 'No',\n        '--indices': [3, 4],\n        '--metadata': 'dfdctrainset/train_data.csv',\n        '--path': 'dfdctrainset/train_set_faces/train_set_faces/'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"zzRvg8aTUIbh"},"cell_type":"code","source":"# frame evaluation\nargs = {'--mode': 'predict frame',\n        '--path': 'dfdctrainset/train_set_faces/train_set_faces/dfdc_train_part_0/acagallncj.png'}\nargs = {'--mode': 'predict frame',\n        '--path': 'Nixon FAKE.png'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8enrOkxKUIbk","outputId":"49d2b735-03dd-49bc-dee9-cc3177c40b9d"},"cell_type":"code","source":"if __name__ == \"__main__\":\n    out = main(**args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"5GUSYAUHUIbq"},"cell_type":"code","source":"train_set, valid_set = train_valid_split('dfdctrainset/train_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GPgDeMcwUIbs","outputId":"15674a5f-b75a-4ab9-96dd-9c1a653ec342"},"cell_type":"code","source":"sample =valid_set.sample()\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"daKUOfIQUIbv","outputId":"846c1409-f2ef-4bbc-c44c-9cfbc217973c"},"cell_type":"code","source":"real = valid_set[valid_set.video == sample.original.values[0]]\nreal","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}